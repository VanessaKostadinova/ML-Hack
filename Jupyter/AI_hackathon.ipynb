{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AI hackathon.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PWxHlbbQZ9su"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('data.csv')\n",
        "X = df.loc[:, df.columns != 'sentiment'].to_numpy()\n",
        "y = df[\"sentiment\"].to_numpy()"
      ],
      "metadata": {
        "id": "pMiJ_yFiaYGN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Library for splitting data\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split dataset into training set and test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y, random_state = 50, test_size = 0.25)\n",
        "\n",
        "# print the total number of training set (records)\n",
        "print(np.shape(y_train))\n",
        "\n",
        "# print the total number of test set (records)\n",
        "print(np.shape(y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pEGR08oZcDSB",
        "outputId": "876becc9-216e-4a89-f994-7fecb892eded"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3164,)\n",
            "(1055,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Library for KNeighborsClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "# Create a training model KNeighborsClassifier\n",
        "model_1 = KNeighborsClassifier()\n",
        "\n",
        "#Create the SVM model\n",
        "from sklearn.svm import SVC\n",
        "model_2 = SVC(kernel = 'poly', random_state = 0)\n",
        "\n",
        "# Creating Decision Tree Classifier\n",
        "from sklearn import tree\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "model_3 = DecisionTreeClassifier()\n",
        "\n",
        "#Importing our MLPClassifier model\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "#model initialization\n",
        "model_4 = MLPClassifier(hidden_layer_sizes=(10,5), max_iter=2000, alpha=0.01, #try change hidden layer\n",
        "                     solver='sgd', verbose=0,  random_state=121) #try verbode=0 to train with out logging"
      ],
      "metadata": {
        "id": "FSmVzM3pcLw1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_1.fit(X_train, y_train)\n",
        "model_2.fit(X_train, y_train)\n",
        "model_3.fit(X_train, y_train)\n",
        "model_4.fit(X_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5gjeeKDLcQJt",
        "outputId": "9f2a4d2c-5baa-4c81-f202-1d7898d5ebc5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPClassifier(alpha=0.01, hidden_layer_sizes=(10, 5), max_iter=2000,\n",
              "              random_state=121, solver='sgd', verbose=0)"
            ]
          },
          "metadata": {},
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "print(\"model 1\")\n",
        "print(model_1.score(X_test, y_test))\n",
        "print(f1_score(y_test, model_1.predict(X_test)))\n",
        "print(confusion_matrix(y_test, model_1.predict(X_test)))\n",
        "print(\"model 2\")\n",
        "print(model_2.score(X_test, y_test))\n",
        "print(f1_score(y_test, model_2.predict(X_test)))\n",
        "print(confusion_matrix(y_test, model_2.predict(X_test)))\n",
        "print(\"model 3\")\n",
        "print(model_3.score(X_test, y_test))\n",
        "print(f1_score(y_test, model_3.predict(X_test)))\n",
        "print(confusion_matrix(y_test, model_3.predict(X_test)))\n",
        "print(\"model 4\")\n",
        "print(model_4.score(X_test, y_test))\n",
        "print(f1_score(y_test, model_4.predict(X_test)))\n",
        "print(confusion_matrix(y_test, model_4.predict(X_test)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D1BX0pz8c1t0",
        "outputId": "e89a2616-0b42-4cc1-c93a-509fa5c5f5a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model 1\n",
            "0.7706161137440758\n",
            "0.8561236623067776\n",
            "[[ 93 115]\n",
            " [127 720]]\n",
            "model 2\n",
            "0.8170616113744076\n",
            "0.8967362225789193\n",
            "[[ 24 184]\n",
            " [  9 838]]\n",
            "model 3\n",
            "0.8218009478672986\n",
            "0.8937853107344633\n",
            "[[ 76 132]\n",
            " [ 56 791]]\n",
            "model 4\n",
            "0.804739336492891\n",
            "0.891005291005291\n",
            "[[  7 201]\n",
            " [  5 842]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "NLTK testing"
      ],
      "metadata": {
        "id": "3MP5_kX5xfjD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pprint import pprint\n",
        "import re, unicodedata\n",
        "from nltk.stem import WordNetLemmatizer"
      ],
      "metadata": {
        "id": "3c0X-Q05whBi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download([\"punkt\"])\n",
        "nltk.download('stopwords')\n",
        "nltk.download('vader_lexicon')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
        "lemmatizer = WordNetLemmatizer()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PfGcvb060xQx",
        "outputId": "3e9f1abf-9e8f-45da-8aa8-2885598d83b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('dataraw.csv')\n",
        "listA = df[\"text\"].values.tolist()\n",
        "sentiment = df[\"sentiment\"].values.tolist()\n",
        "print(len(listA))\n",
        "print(len(sentiment))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B1Rc-i9qx66h",
        "outputId": "67ae0600-2db2-4b2d-d038-97a10403b021"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4227\n",
            "4227\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "listA[21]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "Jq1orEj7T80M",
        "outputId": "2d0b57a3-1d8f-486a-bea9-2493ee2a5e02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Ok, let’s see... \\n\\nA ton of water \\nNon-perishable foods \\nAmple beer and whiskey \\nCandles and battery powered reading lights \\nMore books than anyone could read in multiple lifetimes \\nJack the Cat \\n\\nOk Hurricane Michael, I’m ready to take you on.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 123
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "listB = []\n",
        "listC = []\n",
        "for i in listA:\n",
        "  text = i.lower()\n",
        "  text = re.sub(r'http\\S+', '', text)\n",
        "  text = re.sub(r'(\\s)@\\w+', r'\\1', text)\n",
        "  words = nltk.word_tokenize(text)\n",
        "  words = [w for w in words if w.isalpha()]\n",
        "  words = [w for w in words if w not in stopwords]\n",
        "  words = [lemmatizer.lemmatize(w) for w in words]\n",
        "  #pprint(words, width=79, compact=True)\n",
        "  listB.append(words)\n",
        "  listC.extend(words)"
      ],
      "metadata": {
        "id": "EZA4R6aqzLXw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(listB))\n",
        "print(len(listC))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7x23bdNu1dFz",
        "outputId": "e1113638-ec93-4454-bf00-fd8ceb474fc1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4227\n",
            "51783\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fd = nltk.FreqDist(listC)\n",
        "fd.tabulate(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s8-3MJ4M0pdS",
        "outputId": "34613889-97f9-4f54-f866-5f7de85526df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hurricane   michael      food  survivor      fema   furious       via     water     daily     beast \n",
            "     4374      4094      3277      2392      2377      2078      1052       752       683       671 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "sia = SentimentIntensityAnalyzer()"
      ],
      "metadata": {
        "id": "hzGM42e86EwU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sia_list = list()\n",
        "for i in listA:\n",
        "  sia_list.append(sia.polarity_scores(i)[\"compound\"])"
      ],
      "metadata": {
        "id": "GSS2f_Sr6ucT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test = [i for i in sentiment if i == 0]\n",
        "print(len(test))\n",
        "print(len(sentiment))\n",
        "percent = len(test) / len(sentiment)\n",
        "print(percent)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NmTHw2Gm9mS-",
        "outputId": "0b5deedc-49eb-4a00-97a7-ce2d33a647db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "888\n",
            "4227\n",
            "0.2100780695528744\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "s0_listB = list()\n",
        "s1_listB = list()\n",
        "s0_listC = list()\n",
        "s1_listC = list()\n",
        "\n",
        "for i in range(len(sentiment)):\n",
        "  if (sentiment[i] == 0):\n",
        "    s0_listB.append(listB[i])\n",
        "    s0_listC.extend(listB[i])\n",
        "  elif (sentiment[i] == 1):\n",
        "    s1_listB.append(listB[i])\n",
        "    s1_listC.extend(listB[i])"
      ],
      "metadata": {
        "id": "R3GOR9MrOAO2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s0_fd = nltk.FreqDist(s0_listC)\n",
        "s0_fd.tabulate(10)\n",
        "s1_fd = nltk.FreqDist(s1_listC)\n",
        "s1_fd.tabulate(10)\n",
        "\n",
        "top_100_s0 = {word for word, count in s0_fd.most_common(100)}\n",
        "top_100_s1 = {word for word, count in s1_fd.most_common(100)}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9-aiNi8hPgbA",
        "outputId": "b574cfde-a7ea-49b5-9ca4-77f53939cf48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hurricane   michael      food     water      fema  survivor   furious       car   florida      help \n",
            "      958       902       609       231       229       215       177       167       134       128 \n",
            "hurricane   michael      food  survivor      fema   furious       via     daily     beast     water \n",
            "     3416      3192      2668      2177      2148      1901       970       614       608       521 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(s0_fd.most_common(100))\n",
        "print(top_100_s1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tv1lLvH1WqKH",
        "outputId": "e03b1cce-a6c3-461a-cc7d-0283fe7b8cbc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('hurricane', 958), ('michael', 902), ('food', 609), ('water', 231), ('fema', 229), ('survivor', 215), ('furious', 177), ('car', 167), ('florida', 134), ('help', 128), ('ride', 112), ('trump', 100), ('storm', 99), ('power', 97), ('via', 82), ('people', 81), ('safe', 69), ('daily', 69), ('area', 68), ('please', 64), ('beast', 63), ('get', 61), ('need', 61), ('city', 60), ('affected', 59), ('u', 57), ('safety', 55), ('panama', 55), ('like', 54), ('tip', 53), ('victim', 51), ('supply', 50), ('day', 50), ('good', 48), ('path', 47), ('free', 46), ('see', 45), ('panhandle', 45), ('one', 45), ('many', 45), ('rally', 45), ('take', 44), ('going', 44), ('vehicle', 44), ('usda', 42), ('ready', 41), ('tree', 40), ('video', 39), ('right', 39), ('offer', 39), ('time', 38), ('hit', 37), ('first', 37), ('without', 36), ('emergency', 36), ('stay', 35), ('go', 35), ('home', 35), ('everyone', 33), ('disaster', 31), ('keep', 31), ('busy', 31), ('look', 31), ('house', 31), ('family', 30), ('battery', 30), ('well', 29), ('medicine', 28), ('hope', 28), ('still', 28), ('beach', 28), ('damage', 27), ('thanks', 27), ('fl', 27), ('away', 27), ('shelter', 27), ('know', 27), ('live', 26), ('made', 26), ('news', 26), ('due', 25), ('great', 25), ('service', 25), ('left', 25), ('make', 24), ('much', 24), ('love', 24), ('impacted', 24), ('new', 23), ('truck', 23), ('back', 23), ('getting', 23), ('today', 22), ('prepare', 22), ('friend', 22), ('resident', 22), ('president', 22), ('move', 22), ('wind', 22), ('update', 21)]\n",
            "{'car', 'marshal', 'via', 'u', 'food', 'get', 'please', 'steal', 'would', 'report', 'mile', 'still', 'need', 'remotely', 'puerto', 'water', 'getting', 'fl', 'area', 'week', 'help', 'trump', 'supply', 'job', 'president', 'one', 'safety', 'across', 'rico', 'hurricane', 'extends', 'wake', 'video', 'care', 'city', 'say', 'scramble', 'looter', 'hit', 'go', 'vehicle', 'battery', 'without', 'take', 'beast', 'like', 'south', 'missing', 'fire', 'death', 'ride', 'trying', 'people', 'panama', 'shot', 'tesla', 'decided', 'daily', 'florida', 'enforcement', 'surge', 'news', 'going', 'day', 'time', 'failing', 'furious', 'rally', 'toll', 'fema', 'victim', 'foxnews', 'county', 'anything', 'michael', 'flooding', 'killed', 'panhandle', 'rise', 'mexico', 'law', 'affected', 'hurricanemichael', 'vote', 'power', 'resident', 'home', 'said', 'tree', 'storm', 'government', 'survivor', 'beach', 'issue', 'relief', 'left', 'family', 'citizen', 'damage', 'today'}\n"
          ]
        }
      ]
    }
  ]
}